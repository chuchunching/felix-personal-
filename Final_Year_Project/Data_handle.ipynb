{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Data Entries: \n",
    "0 Bound, 1 Date + Time, 2 Seq No, 3 Lane, 4 Speed, 5 Class, 6 No of Axle, [ Axle Weight, Axle Spacing]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <span style=\"color:red\"> Methods and Procedures </span>\n",
    "\n",
    "\n",
    "#### Data Cleaning\n",
    "- Sort the data based on Date > Bound > lane   DONE\n",
    "- obtain the Gap Time    DONE\n",
    "\n",
    "#### Fitting and Regression\n",
    "- Find the daily average vehicle flow  DONE\n",
    "- Find the Variation in Traffic flow across different times of the day DONE\n",
    "- Find the variation in LDV:HDV ratio across different times of the day  GAVE UP\n",
    "- Find the variation in (Traffic Volume in hour)/(Traffic Volume in day) across different times of the day [Reference](https://medium.com/hal24k-techblog/a-guide-to-generating-probability-distributions-with-neural-networks-ffc4efacd6a4) \n",
    "- Find the PDF of the vehicle weight of LDV  DONE \n",
    "- Find the PDF of the vehicle weight of HDV  DONE\n",
    "- Find the Relation between parameters with the below methods  \n",
    "\n",
    "#### Monte Carlo Simluation\n",
    "- Set up an environment for simulation. Refer to your IR. \n",
    "- Run for 2400 years (USE SEED to make sure the value is same every time) -> maybe do it a few times and take the average?\n",
    "- Obtain max load effect. -> Transition to SAP2000\n",
    "- Then maybe measure the load effect with varying parameters to plot a 3d surface. (but what parameters should I vary with?)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship between each entries: \n",
    " - Time of the day -> Proportion of Vehicle Class\n",
    "   - Vehicle Class -> Vehicle Speed ? Need to see if there is a relationship or not. \n",
    "      - Vehicle Class + Vehicle Speed -> Gap Time  \n",
    "        //\n",
    "   - Vehicle Class -> Axle Number \n",
    "      - Axle number -> Total Weight & Axle Weight\n",
    "\n",
    "Multiple Lasso/Ridge Regression or polynomial regression would be used to draw the relationships of the above figures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions and Relationships: \n",
    "- the Traffic composition relates to the time of the day\n",
    "- Gap Time between vehicles is determined by the vehicle speed and the vehicle class ie. weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data():\n",
    "    all_data= pd.read_excel('output.xlsx')\n",
    "    return all_data\n",
    "# usecols='A:I'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculates the Gap Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_gap_distance():\n",
    "    all_data = import_data()\n",
    "    # all_data.loc[:,'Seq No':'Class'].head()\n",
    "    all_data['Date + Time'] = pd.to_datetime(all_data['Date + Time'])\n",
    "    all_data['Date'] = pd.to_datetime(all_data['Date + Time']).dt.date\n",
    "    all_data['Date'].unique()\n",
    "\n",
    "    all_data = all_data.sort_values(['Date','Bound', 'Lane','Seq No'])\n",
    "\n",
    "    all_data['Same Bound'] = all_data['Bound'] == all_data['Bound'].shift(1)\n",
    "    all_data['Same Lane'] = all_data['Lane'] == all_data['Lane'].shift(1)    \n",
    "    # if all_data.loc['Bound'].eq(all_data.loc['Bound'].shift(1)) and all_data.loc['Lane'].eq(all_data.loc['Lane'].shift(1)):\n",
    "    #     all_data['Gap Time'] = all_data['Speed'] * (all_data['Date + Time'].shift(1) - all_data['Date + Time'])\n",
    "    # else:\n",
    "    #     all_data['Gap Time'] = pd.NA\n",
    "\n",
    "    filter1 = all_data['Same Bound'] == True\n",
    "    filter2 = all_data['Same Lane'] == True\n",
    "    not_filter1 = all_data['Same Bound'] == False\n",
    "    not_filter2 = all_data['Same Lane'] == False\n",
    "\n",
    "    change = all_data['Date + Time'].diff().dt.seconds\n",
    "    # all_data['Speed'] * (all_data['Date + Time'].shift(1) - all_data['Date + Time'])\n",
    "\n",
    "    all_data.loc[filter1 & filter2, 'Gap Time'] = change\n",
    "    all_data.loc[not_filter1, 'Gap Time'] = pd.NA\n",
    "    all_data.loc[not_filter2, 'Gap Time'] = pd.NA\n",
    "\n",
    "\n",
    "    # all_data = all_data.fillna(all_data['Gap Time'].mean())\n",
    "\n",
    "    cols = ['Gap Time']\n",
    "    all_data.loc[:,cols] = all_data.loc[:,cols].bfill()\n",
    "    return \n",
    "# cal_gap_distance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset = all_data.drop(columns=['Date + Time','Date']).loc[:,:'No of Axle']\n",
    "# Weight_Gap_Corr = subset.corr().round(2)\n",
    "# Weight_Gap_Corr = Weight_Gap_Corr.abs()\n",
    "\n",
    "# def highlight_greater_than_7(val):\n",
    "#     \"\"\"\n",
    "#     Takes a scalar and returns a string with\n",
    "#     the css property 'background-color: yellow' for\n",
    "#     values greater than 80, black otherwise.\n",
    "#     \"\"\"\n",
    "#     color = 'green' if float(val) >= 0.7 else 'grey'\n",
    "#     return f'background-color: {color}'\n",
    "\n",
    "# Weight_Gap_Corr = Weight_Gap_Corr.style.applymap(highlight_greater_than_7)\n",
    "\n",
    "\n",
    "# Weight_Gap_Corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that there is not much relationship between Gap distace and Total Weight\n",
    "\n",
    "There is relationship between speed and lane number, but should not affect the calculation of load effect. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class 1 - Motor cycles\n",
    "\n",
    "Class 2 - Private cars\n",
    "\n",
    "\n",
    "Class 3 - Light buses\n",
    "\n",
    "Class 4 - Light Goods Vehicles\n",
    "\n",
    "Class 5 - Medium Goods Vehicles\n",
    "\n",
    "Class 6 - Rigid Heavy Goods Vehicles\n",
    "\n",
    "Class 7 - Articulated Heavy Goods Vehicles\n",
    "\n",
    "Class 8 - Buses and Coaches\n",
    "\n",
    "Class 9 - Unclassified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Vehicular Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_veh_flow():\n",
    "    date_list = all_data['Date'].unique()\n",
    "    avf_list = []\n",
    "    for date in date_list: \n",
    "        avf_list.append(round(len(all_data[all_data['Date']==date].index),2))\n",
    "    dict = {'Date': date_list, \"Daily Vehicular Flow\": avf_list}\n",
    "    AVFdf = pd.DataFrame(dict)\n",
    "    print(AVFdf)\n",
    "    mean_vf = AVFdf[\"Daily Vehicular Flow\"].mean()\n",
    "    sd_vf = AVFdf[\"Daily Vehicular Flow\"].std()\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def violin_plot_speed_and_gap_distance():\n",
    "    date_list = all_data['Date'].unique()\n",
    "\n",
    "    from statsmodels.graphics.boxplots import violinplot\n",
    "    \n",
    "    speed_list = []\n",
    "    gap_list = []\n",
    "    for date in date_list:\n",
    "        speed_list.append(np.array(all_data[all_data['Date']==date]['Speed']))\n",
    "        gap_list.append(np.array(all_data[all_data['Date']==date]['Gap Time']))\n",
    "\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "    violinplot(data=speed_list, ax=ax1, labels=date_list, side='right', show_boxplot=False)\n",
    "    violinplot(data=gap_list, ax=ax2, labels=date_list, side='right', show_boxplot=False)\n",
    "    # plt.xlabel('Date')\n",
    "    # plt.ylabel('Speed (km/h)')\n",
    "    plt.setp(ax1, ylim=[0,150], xlabel = 'Date', ylabel = 'Speed (km/h)')\n",
    "    # plt.setp(ax2, ylim=[0,20000], xlabel = 'Date', ylabel = 'Gap Time (m)')\n",
    "    plt.suptitle('Violin Plot of Speed and Gap Time')\n",
    "    fig.tight_layout()\n",
    "    fig.autofmt_xdate()    \n",
    "\n",
    "    fig, ax1 = plt.subplots(1,1)\n",
    "    violinplot(data=speed_list,ax=ax1 , labels=date_list, side='right', show_boxplot=False)\n",
    "    plt.title('Violin Plot of Speed Against Different Days')\n",
    "    plt.setp(ax1, ylim=[0,150], xlabel = 'Date', ylabel = 'Speed (km/h)')\n",
    "    fig.tight_layout()\n",
    "    fig.autofmt_xdate()\n",
    "    plt.show()\n",
    "\n",
    "    fig, ax1 = plt.subplots(1,1)\n",
    "    violinplot(data=gap_list, ax=ax1, labels=date_list, side='right', show_boxplot=False)\n",
    "    plt.setp(ax1, ylim=[0,20000], xlabel = 'Date', ylabel = 'Gap Time (m)')\n",
    "    plt.title('Violin Plot of Gap Time Against Different Days')\n",
    "    fig.tight_layout()\n",
    "    fig.autofmt_xdate()\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hourly Vehicular Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This assumes that all data are measured from the same bridge. Since for different bridges, the number of lanes would affect the expected vehicular flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### in Unit of Minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import time as t\n",
    "def generate_time(time_list):\n",
    "    for h in range(24):\n",
    "        for m in range(60):\n",
    "            time_list.append(t(h,m).strftime(\"%H:%M\"))\n",
    "    return\n",
    "\n",
    "\n",
    "def vehicular_flow_in_minutes_setup():\n",
    "    # Create the Time column, where it is in the string format of hour:minute in 1 minute increments\n",
    "    all_data['Time'] = all_data['Date + Time'].dt.round('1min')\n",
    "    all_data['Time'] = all_data['Time'].dt.strftime(\"%H:%M\")\n",
    "\n",
    "    # Make a time list to store all the x data from 00:00 to 23:59\n",
    "    time_list = []\n",
    "    generate_time(time_list)\n",
    "    dict = {\"Time\": time_list}\n",
    "\n",
    "    # Loops over the number of recorded dates\n",
    "    total_entries = 0\n",
    "    for date in date_list:\n",
    "        vehicle_flow = []\n",
    "        # Loops over each minute\n",
    "        for time in time_list:\n",
    "            # Tallies the number of vehicle that is recorded in that minute. -> convert the unit from veh/min to veh/hr\n",
    "            # ? Times 60 for the total numebr of cars to convert: cars per min to cars per hour\n",
    "            vehicle_flow.append(len(all_data[(all_data['Date']==date) & (all_data['Time']==time)].index) * 60)\n",
    "\n",
    "        dict[date] = vehicle_flow\n",
    "        for item in vehicle_flow:\n",
    "            total_entries += item / 60\n",
    "\n",
    "    VF_testing = pd.DataFrame(dict)\n",
    "    return VF_testing, time_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vehicular_flow_in_minutes():\n",
    "    VF_testing, time_list = vehicular_flow_in_minutes_setup()\n",
    "    for date in date_list:\n",
    "        # removes the time data for when there is no vehicle recorded\n",
    "        tmp = np.array(VF_testing[date].astype(float))\n",
    "        tmp[tmp==0] = np.nan\n",
    "        plt.plot(VF_testing['Time'], VF_testing[date], label=date)\n",
    "    plt.xticks(time_list[::60], rotation=70)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.xlabel('Time of Day')\n",
    "    plt.ylabel('Vehicular Flow (veh/h)')\n",
    "    plt.title('Variations in Vehicular Flow Across Different Times of Day')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return\n",
    "# vehicular_flow_in_minutes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def vehicular_flow_in_minutes_scatter_plot():\n",
    "    VF_testing, time_list = vehicular_flow_in_minutes_setup()\n",
    "\n",
    "    for date in date_list:\n",
    "        tmp = np.array(VF_testing[date].astype(float))\n",
    "        tmp[tmp==0] = np.nan\n",
    "        plt.scatter(VF_testing['Time'], tmp, label=date)\n",
    "        # plt.plot(VF_testing['Time'], VF_testing[date], label=date)\n",
    "    plt.xticks(time_list[::60], rotation=70)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.xlabel('Time of Day')\n",
    "    plt.ylabel('Vehicular Flow (veh/h)')\n",
    "    plt.title('Variations in Vehicular Flow Across Different Times of Day')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return\n",
    "# vehicular_flow_in_minutes_scatter_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Unit of Hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import time as t\n",
    "\n",
    "def vehicular_flow_in_hours_setup():\n",
    "        \n",
    "    def generate_time(time_list):\n",
    "        for h in range(24):\n",
    "            time_list.append(t(h).strftime(\"%H\"))\n",
    "        return\n",
    "\n",
    "    # Create the Time column, where it is in the string format of hour:minute in 1 minute increments\n",
    "    all_data['Time'] = all_data['Date + Time'].dt.floor('h')\n",
    "    all_data['Time'] = all_data['Time'].dt.strftime(\"%H\")\n",
    "\n",
    "    # Make a time list to store all the x data from 00:00 to 23:59\n",
    "    time_list = []\n",
    "    generate_time(time_list)\n",
    "    dict = {\"Time\": time_list}\n",
    "\n",
    "    # Loops over the number of recorded dates\n",
    "    for date in date_list:\n",
    "        vehicle_flow = []\n",
    "        # Loops over each minute\n",
    "        for time in time_list:\n",
    "            # Tallies the number of vehicle that is recorded in that minute. -> convert the unit from veh/min to veh/hr\n",
    "            # ? Times 60 for the total numebr of cars to convert: cars per min to cars per hour\n",
    "            tmp = all_data[(all_data['Date']==date) & (all_data['Time']==time)]\n",
    "            hourly_flow = len(tmp.index)\n",
    "            if hourly_flow < 100: \n",
    "                hourly_flow = 0\n",
    "            vehicle_flow.append(hourly_flow)\n",
    "        dict[date] = vehicle_flow\n",
    "    # print()\n",
    "    VF_testing = pd.DataFrame(dict)\n",
    "\n",
    "    # Finding the Average Values\n",
    "    VF_testing['Total'] = VF_testing.iloc[:,1:6].sum(axis=1)\n",
    "    VF_testing['Unique Dates'] = VF_testing.iloc[:,1:6].astype(bool).sum(axis=1)\n",
    "\n",
    "    VF_testing['Average'] = VF_testing['Total'] / VF_testing['Unique Dates'] \n",
    "    \n",
    "    return VF_testing, time_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vehicular_flow_in_hours():\n",
    "    from datetime import time as t\n",
    "\n",
    "    VF_testing, time_list = vehicular_flow_in_hours_setup()\n",
    "    print(len(time_list))\n",
    "    # Print one with the 0 value\n",
    "    for date in date_list:\n",
    "        # removes the time data for when there is no vehicle recorded\n",
    "        tmp = np.array(VF_testing[date].astype(float))\n",
    "        tmp[tmp==0] = np.nan\n",
    "        \n",
    "        plt.plot(VF_testing['Time'], VF_testing[date], label=date)\n",
    "        # plt.plot(VF_testing['Time'], VF_testing[date], label=date)\n",
    "\n",
    "    plt.xticks(time_list, rotation=70)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.xlabel('Time of Day')\n",
    "    plt.ylabel('Vehicular Flow (veh/h)')\n",
    "    plt.title('Variations in Vehicular Flow Across Different Times of Day')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Print one without 0 values\n",
    "    date_list = all_data['Date'].unique()\n",
    "    for date in date_list:\n",
    "        # removes the time data for when there is no vehicle recorded\n",
    "        tmp = np.array(VF_testing[date].astype(float))\n",
    "        tmp[tmp==0] = np.nan\n",
    "        \n",
    "        plt.plot(VF_testing['Time'], tmp, label=date)\n",
    "        # plt.plot(VF_testing['Time'], VF_testing[date], label=date)\n",
    "\n",
    "    plt.xticks(time_list, rotation=70)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.xlabel('Time of Day')\n",
    "    plt.ylabel('Vehicular Flow (veh/h)')\n",
    "    plt.title('Variations in Vehicular Flow Across Different Times of Day')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def vehicular_flow_in_hours_scatter_plot():\n",
    "\n",
    "    VF_testing, time_list = vehicular_flow_in_hours_setup()\n",
    "    # Scatter Plot\n",
    "    for date in date_list:\n",
    "        # removes the time data for when there is no vehicle recorded\n",
    "        tmp = np.array(VF_testing[date].astype(float))\n",
    "        tmp[tmp==0] = np.nan\n",
    "        \n",
    "        plt.scatter(VF_testing['Time'], tmp, label=date)\n",
    "        # plt.plot(VF_testing['Time'], VF_testing[date], label=date)\n",
    "\n",
    "    plt.xticks(time_list, rotation=70)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.xlabel('Time of Day')\n",
    "    plt.ylabel('Vehicular Flow (veh/h)')\n",
    "    plt.title('Variations in Vehicular Flow Across Different Times of Day')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "# vehicular_flow_in_hours()\n",
    "# vehicular_flow_in_hours_scatter_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now Taking the Average Value across each day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_time(time_list):\n",
    "#     for h in range(24):\n",
    "#         time_list.append(t(h).strftime(\"%H\"))\n",
    "#     return\n",
    "\n",
    "# # Create the Time column, where it is in the string format of hour:minute in 1 minute increments\n",
    "# all_data['Time'] = all_data['Date + Time'].dt.floor('h')\n",
    "# all_data['Time'] = all_data['Time'].dt.strftime(\"%H\")\n",
    "\n",
    "# # Make a time list to store all the x data from 00:00 to 23:59\n",
    "# time_list = []\n",
    "# generate_time(time_list)\n",
    "# dict = {\"Time\": time_list}\n",
    "\n",
    "# vehicle_flow = []\n",
    "# # Loops over each minute\n",
    "# for time in time_list:\n",
    "#     # Tallies the number of vehicle that is recorded in that minute. -> convert the unit from veh/min to veh/hr\n",
    "#     # ? Times 60 for the total numebr of cars to convert: cars per min to cars per hour\n",
    "#     tmp = all_data[all_data['Time']==time]\n",
    "#     no_of_dates_involved = len(tmp['Date'].unique())\n",
    "#     if no_of_dates_involved != 0:\n",
    "#         hourly_flow_value = len(tmp.index) / no_of_dates_involved\n",
    "#     vehicle_flow.append(hourly_flow_value)\n",
    "#     dict['Vehicle Flow'] = vehicle_flow\n",
    "\n",
    "# VF_testing = pd.DataFrame(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vehicular_flow_in_hours_with_average_and_scatter():\n",
    "\n",
    "    VF_testing, time_list = vehicular_flow_in_hours_setup()\n",
    "\n",
    "    # graph plot the average value\n",
    "    plt.plot(time_list, VF_testing['Average'],'.-', label='Average')\n",
    "\n",
    "\n",
    "    # Scatter Plot\n",
    "    for date in date_list:\n",
    "        # removes the time data for when there is no vehicle recorded\n",
    "        tmp = np.array(VF_testing[date].astype(float))\n",
    "        tmp[tmp==0] = np.nan\n",
    "        \n",
    "        plt.scatter(VF_testing['Time'], tmp, label=date)\n",
    "\n",
    "    plt.xticks(time_list, rotation=70)\n",
    "    plt.legend(loc=0)\n",
    "    plt.grid()\n",
    "    plt.xlabel('Time of Day (24 hour format)')\n",
    "    plt.ylabel('Vehicular Flow (veh/h)')\n",
    "    plt.title('Variations in Hourly Vehicular Flow Across Different Times of Day')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return \n",
    "\n",
    "# vehicular_flow_in_hours_with_average_and_scatter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Defining Data\n",
    "# VF_testing, time_list = vehicular_flow_in_hours_setup()\n",
    "# tmp = VF_testing.dropna(subset=[\"Average\"])\n",
    "\n",
    "# # Building NN \n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "\n",
    "# X_train       = tmp['Time'].astype(float)\n",
    "# y_train       = tmp['Average'].astype(float)\n",
    "\n",
    "# # essential preprocessing: imputation; substitute any 'NaN' with mean value \n",
    "# X_train      = X_train.fillna(X_train.mean())\n",
    "\n",
    "\n",
    "# # parameters for keras\n",
    "# input_dim        =  1 # number of neurons in the input layer\n",
    "# n_neurons        = 100       # number of neurons in the first hidden layer\n",
    "# # epochs           = 1000      # number of training cycles\n",
    "# epochs           = 1      # number of training cycles\n",
    "\n",
    "# # keras model\n",
    "# model = Sequential()        # a model consisting of successive layers\n",
    "# # input layer\n",
    "# model.add(Dense(n_neurons, input_dim=input_dim, \n",
    "#                 kernel_initializer='normal', \n",
    "#                 activation='relu'))\n",
    "# # output layer, with one neuron\n",
    "# model.add(Dense(n_neurons, activation = 'relu'))\n",
    "# model.add(Dense(n_neurons, activation = 'relu'))\n",
    "# model.add(Dense(n_neurons, activation = 'relu'))\n",
    "# model.add(Dense(n_neurons, activation = 'sigmoid'))\n",
    "# model.add(Dense(n_neurons, activation = 'relu'))\n",
    "# model.add(Dense(n_neurons, activation = 'relu'))\n",
    "# model.add(Dense(n_neurons, activation = 'relu'))\n",
    "# model.add(Dense(n_neurons, activation = 'sigmoid'))\n",
    "# model.add(Dense(n_neurons, activation = 'relu'))\n",
    "# model.add(Dense(n_neurons, activation = 'relu'))\n",
    "# model.add(Dense(n_neurons, activation = 'relu'))\n",
    "# model.add(Dense(n_neurons, activation = 'relu'))\n",
    "# model.add(Dense(1, kernel_initializer='normal'))\n",
    "# # compile the model\n",
    "# model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# # train the model\n",
    "# history = model.fit(X_train,y_train, epochs=epochs, verbose=1, validation_split=0.1)\n",
    "\n",
    "# # use the model to predict the prices for the test data\n",
    "# y_predicted = model.predict(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute the output \n",
    "# y_predicted = model.predict(X_train)\n",
    "\n",
    "# # X_train = sc.inverse_transform(X_train)\n",
    "\n",
    "# # Display the result\n",
    "# plt.plot(X_train, y_train)\n",
    "# plt.plot(X_train, y_predicted, 'r', linewidth=4)\n",
    "# plt.xticks(VF_testing['Time'].astype(int), rotation=70)\n",
    "# plt.legend(loc=0)\n",
    "# plt.grid()\n",
    "# plt.xlabel('Time of Day (24 hour format)')\n",
    "# plt.ylabel('Vehicular Flow (veh/h)')\n",
    "# plt.title('Variations in Hourly Vehicular Flow Across Different Times of Day')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Extract the loss values from the training history\n",
    "# train_loss = history.history['loss']\n",
    "# val_loss = history.history['val_loss']\n",
    "\n",
    "# # Plot the learning curve\n",
    "# plt.plot(range(1, len(train_loss) + 1), train_loss, label='Training Loss')\n",
    "# plt.plot(range(1, len(val_loss) + 1), val_loss, label='Validation Loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Learning Curve')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ABANDONED (LDV : HDV at Varying Times of the Day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(time_list[::60])\n",
    "# dict = {\"Time\": time_list}\n",
    "\n",
    "# all_data['Hour'] = all_data['Date + Time'].dt.round('1h')\n",
    "# all_data['Hour'] = all_data['Hour'].dt.strftime(\"%H:%M\")\n",
    "\n",
    "\n",
    "# for date in date_list:\n",
    "#     LDV_vehicle_flow = []\n",
    "#     HDV_vehicle_flow = []\n",
    "#     for time in time_list:\n",
    "#         # ? Times 60 for the total numebr of cars to convert: cars per min to cars per hour\n",
    "#         LDV_vehicle_flow.append(len(all_data[(all_data['Date']==date) & (all_data['Hour']==time) & (all_data['Class'].astype(int)<=2)].index) * 60)\n",
    "#         HDV_vehicle_flow.append(len(all_data[(all_data['Date']==date) & (all_data['Hour']==time) & (all_data['Class'].astype(int)>2)].index) * 60)\n",
    "\n",
    "#     dict[date.strftime(\"%Y%M%D\") + 'LDV'] = LDV_vehicle_flow\n",
    "#     dict[date.strftime(\"%Y%M%D\") + 'HDV'] = HDV_vehicle_flow\n",
    "\n",
    "#     print(len(vehicle_flow))\n",
    "\n",
    "#     LDV_HDV_testing = pd.DataFrame(dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import time as t\n",
    "\n",
    "def generate_time(time_list):\n",
    "    for h in range(24):\n",
    "        for m in range(60):\n",
    "            time_list.append(t(h,m))\n",
    "    return\n",
    "\n",
    "total_list = []\n",
    "time_list = []\n",
    "generate_time(time_list)\n",
    "\n",
    "for date in date_list:\n",
    "    # print(LDV_HDV_testing[date.strftime(\"%Y%M%D\") + 'LDV'])\n",
    "    # print(np.array(LDV_HDV_testing[date.strftime(\"%Y%M%D\") + 'LDV']))\n",
    "    total_list = np.concatenate((total_list, (np.array(LDV_HDV_testing[date.strftime(\"%Y%M%D\") + 'LDV']))))\n",
    "time_list = time_list * len(date_list)\n",
    "\n",
    "print(len(total_list))\n",
    "print(len(time_list))\n",
    "\n",
    "plt.scatter(total_list, time_list, label=date)\n",
    "plt.xticks(time_list[::60], rotation=70)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel('Time of Day')\n",
    "plt.ylabel('Vehicular Flow (veh/h)')\n",
    "plt.title('Variations in Vehicular Flow Across Different Times of Day')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDV to HDV Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_LDV_to_HDV_ratio():\n",
    "    # HDV\n",
    "    HDV = all_data.loc[(all_data['Class'].astype(int) > 2)| ((all_data['Class'].astype(int) <= 2) & (all_data['Total Weight'].astype(int) > 6000))]\n",
    "    total_trucks = HDV[HDV.columns[0]].count()\n",
    "    # LDV - Private Vehicle and Motorbikes\n",
    "    PV = all_data.loc[(all_data['Class'].astype(int) <= 2) & (all_data['Total Weight'].astype(int) >= 1500) & (all_data['Total Weight'].astype(int) <= 6000) ]\n",
    "    # PV['Total Weight'].mode\n",
    "    total_cars = PV[PV.columns[0]].count()\n",
    "\n",
    "    # LDV - Motorbike\n",
    "    MB = all_data.loc[(all_data['Class'].astype(int) <= 2) & (all_data['Total Weight'].astype(int) < 1500) ]\n",
    "    total_motorbikes = MB[MB.columns[0]].count()\n",
    "\n",
    "    print([total_trucks,total_cars,total_motorbikes])\n",
    "    print(sum([total_trucks,total_cars,total_motorbikes]))\n",
    "    return HDV, PV, MB, total_trucks,total_cars,total_motorbikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def show_pie_chart():\n",
    "    HDV, PV, MB, total_trucks,total_cars,total_motorbikes = find_LDV_to_HDV_ratio()\n",
    "    pie_chart_labels = ['HDV','LDV - cars','LDV - motorbikes']\n",
    "    myexplode = [0.2,0,0]\n",
    "\n",
    "    print([total_trucks,total_cars,total_motorbikes])\n",
    "\n",
    "    plt.pie(np.array([total_trucks,total_cars,total_motorbikes]),labels = pie_chart_labels, explode = myexplode, shadow = True, autopct='%1.0f%%')\n",
    "    plt.legend(title = \"\", bbox_to_anchor=(1,0), loc=\"lower right\", bbox_transform=plt.gcf().transFigure)\n",
    "    plt.title('Percentage of LDV to HDV')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Private Vehicle Weight Distribution. referenced from [Here](https://colab.research.google.com/drive/11A5Td8nxGSbThzL0NPwwv-E5GpBa0Fv9#scrollTo=sMvpBFj5x1L0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating the Private Vehicle from other data\n",
    "\n",
    "Calculated the Cumulative Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_graph_for_PV():\n",
    "    HDV, PV, MB, total_trucks,total_cars,total_motorbikes = find_LDV_to_HDV_ratio()\n",
    "\n",
    "    PV_total_weight_and_cumper = pd.DataFrame(PV['Total Weight'].value_counts()).reset_index()\n",
    "    PV_total_weight_and_cumper.columns = ['Total Weight', 'Count']\n",
    "    PV_total_weight_and_cumper['Percentage'] = PV_total_weight_and_cumper['Count'] / total_cars\n",
    "    # Before Sorting\n",
    "    x_data1 = np.log10( PV_total_weight_and_cumper['Total Weight'])\n",
    "    x_data = PV_total_weight_and_cumper['Total Weight']\n",
    "    y_data = PV_total_weight_and_cumper['Percentage']\n",
    "    y_data1 = np.log10(PV_total_weight_and_cumper['Percentage'])\n",
    "\n",
    "\n",
    "    plt.title('Probability Density Distribution of Private Vehicle Weight')\n",
    "    plt.scatter(x_data, y_data , s=2)\n",
    "    plt.tight_layout()\n",
    "    plt.xlabel('Gross Vehicle Weight (kg)')\n",
    "    plt.ylabel('Probaility Density')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # ! Sort\n",
    "    PV_total_weight_and_cumper = PV_total_weight_and_cumper.sort_values('Total Weight')\n",
    "    PV_total_weight_and_cumper['Cumulative Probability'] = PV_total_weight_and_cumper['Percentage'].cumsum()\n",
    "\n",
    "\n",
    "    plt.scatter(PV_total_weight_and_cumper['Total Weight'], PV_total_weight_and_cumper['Cumulative Probability'] , s=2)\n",
    "    plt.title('Private Vehicle Total Weight Distribution')\n",
    "    plt.xlabel('Vehicle Total Weight')\n",
    "    plt.ylabel('Cumulative Probability')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train,X_test,y_train,y_test = train_test_split(x_data,y_data,test_size = 0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/felixlaw/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "def train_model_for_private_car():\n",
    "    # PV_total_weight_and_cumper['Total Weight'] = np.log(PV_total_weight_and_cumper['Total Weight'])\n",
    "    X_train       = PV_total_weight_and_cumper['Total Weight']\n",
    "    y_train       = PV_total_weight_and_cumper['Cumulative Probability']\n",
    "\n",
    "    # essential preprocessing: imputation; substitute any 'NaN' with mean value \n",
    "    X_train      = X_train.fillna(X_train.mean())\n",
    "\n",
    "\n",
    "    # parameters for keras\n",
    "    input_dim        =  1 # number of neurons in the input layer\n",
    "    n_neurons        =  50       # number of neurons in the first hidden layer\n",
    "    epochs           = 400       # number of training cycles\n",
    "\n",
    "    # keras model\n",
    "    model = Sequential()        # a model consisting of successive layers\n",
    "    # input layer\n",
    "    model.add(Dense(n_neurons, input_dim=input_dim, \n",
    "                    kernel_initializer='normal', \n",
    "                    activation='relu'))\n",
    "    # output layer, with one neuron\n",
    "    model.add(Dense(n_neurons, activation = 'linear'))\n",
    "    model.add(Dense(n_neurons, activation = 'relu'))\n",
    "    model.add(Dense(n_neurons, activation = 'tanh'))\n",
    "    model.add(Dense(n_neurons, activation = 'relu'))\n",
    "    model.add(Dense(n_neurons, activation = 'relu'))\n",
    "    model.add(Dense(n_neurons, activation = 'relu'))\n",
    "    model.add(Dense(n_neurons, activation = 'relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # compile the model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    # train the model\n",
    "    history = model.fit(y_train,X_train, epochs=epochs, verbose=0,validation_split=0.2)\n",
    "\n",
    "    # use the model to predict the prices for the test data\n",
    "    x_predicted = model.predict(y_train)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "def evaluate_model_private_car():\n",
    "\n",
    "    train_model_for_private_car()\n",
    "    plt=reload(plt)\n",
    "    # Display the result\n",
    "    plt.scatter(X_train, y_train, s=1)\n",
    "    plt.plot(x_predicted, y_train, 'r', linewidth=4)\n",
    "    plt.title('Private Vehicle Total Weight Distribution')\n",
    "    plt.xlabel('Vehicle Total Weight')\n",
    "    plt.ylabel('Cumulative Probability')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # Extract the loss values from the training history\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    # Plot the learning curve\n",
    "    plt=reload(plt)\n",
    "    plt.plot(range(1, len(train_loss) + 1), train_loss, label='Training Loss')\n",
    "    plt.plot(range(1, len(val_loss) + 1), val_loss, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Learning Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(X_train, y_train,validation_data = (X_test,y_test), epochs=300)\n",
    "\n",
    "# plt.plot(history.history['mean_squared_error'])\n",
    "# plt.plot(history.history['val_mean_squared_error'])\n",
    "# plt.title('Model Mean Squared Error')\n",
    "# plt.ylabel('Mean Squared Error')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend(['Train', 'Test'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDV Weight Distribution. referenced from [Here](https://colab.research.google.com/drive/11A5Td8nxGSbThzL0NPwwv-E5GpBa0Fv9#scrollTo=sMvpBFj5x1L0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_graph_for_HDV():\n",
    "\n",
    "    HDV, PV, MB, total_trucks,total_cars,total_motorbikes = find_LDV_to_HDV_ratio()\n",
    "\n",
    "    HDV_total_weight_and_cumper = pd.DataFrame(HDV['Total Weight'].value_counts()).reset_index()\n",
    "    HDV_total_weight_and_cumper.columns = ['Total Weight', 'Count']\n",
    "    HDV_total_weight_and_cumper['Percentage'] = HDV_total_weight_and_cumper['Count'] / total_trucks\n",
    "    # Sort it by Weight First or it messes up the Cumu Prob\n",
    "    HDV_total_weight_and_cumper = HDV_total_weight_and_cumper.sort_values(\"Total Weight\")\n",
    "    HDV_total_weight_and_cumper['Cumulative Probability'] = HDV_total_weight_and_cumper['Percentage'].cumsum()\n",
    "    truck_x_data = HDV_total_weight_and_cumper['Total Weight']\n",
    "    truck_y_data = HDV_total_weight_and_cumper['Percentage']\n",
    "\n",
    "\n",
    "    plt=reload(plt)\n",
    "\n",
    "    plt.scatter(truck_x_data, truck_y_data , s=2)\n",
    "    plt.title('HDV Total Weight Distribution')\n",
    "    plt.xlabel('Vehicle Total Weight')\n",
    "    plt.ylabel('Probability Density')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    truck_x_data = HDV['Total Weight']\n",
    "    truck_y_data = HDV_total_weight_and_cumper['Count']\n",
    "\n",
    "\n",
    "    plt=reload(plt)\n",
    "\n",
    "    plt.hist(x=truck_x_data,bins=1000, density=True)\n",
    "    # truck_x_data = HDV_total_weight_and_cumper['Total Weight']\n",
    "    # plt.scatter(truck_x_data, truck_y_data , s=2)\n",
    "    plt.title('HDV Gross Vehicle Weight Probability Density Distribution')\n",
    "    plt.xlabel('Vehicle Total Weight (kg)')\n",
    "    plt.ylabel('Probabilty Density (%)')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    print(list(HDV_total_weight_and_cumper))\n",
    "\n",
    "\n",
    "    truck_x_data = HDV_total_weight_and_cumper['Total Weight']\n",
    "    truck_y_data = HDV_total_weight_and_cumper['Cumulative Probability']\n",
    "    plt.scatter(truck_x_data, truck_y_data , s=2)\n",
    "    plt.title('HDV Gross Vehicle Weight Cumulative Probability Distribution')\n",
    "    plt.xlabel('Vehicle Total Weight (kg)')\n",
    "    plt.ylabel('Cumulative Probability (%)')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model \n",
    "# Building NN \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "def train_model_for_HDV():\n",
    "    \n",
    "    X_train       = truck_x_data\n",
    "    y_train       = truck_y_data\n",
    "\n",
    "    # essential preprocessing: imputation; substitute any 'NaN' with mean value \n",
    "    X_train      = X_train.fillna(X_train.mean())\n",
    "\n",
    "\n",
    "    # parameters for keras\n",
    "    input_dim        =  1 # number of neurons in the input layer\n",
    "    n_neurons        =  50       # number of neurons in the first hidden layer\n",
    "    epochs           = 400       # number of training cycles\n",
    "\n",
    "    # keras model\n",
    "    model = Sequential()        # a model consisting of successive layers\n",
    "    # input layer\n",
    "    model.add(Dense(n_neurons, input_dim=input_dim, \n",
    "                    kernel_initializer='normal', \n",
    "                    activation='relu'))\n",
    "    # output layer, with one neuron\n",
    "    model.add(Dense(n_neurons, activation = 'relu'))\n",
    "    model.add(Dense(n_neurons, activation = 'tanh'))\n",
    "    model.add(Dense(n_neurons, activation = 'relu'))\n",
    "    model.add(Dense(n_neurons, activation = 'relu'))\n",
    "    model.add(Dense(1))\n",
    "    # compile the model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    # train the model\n",
    "    history = model.fit(y_train,X_train, epochs=epochs, verbose=0, validation_split=0.2)\n",
    "\n",
    "    # use the model to predict the prices for the test data\n",
    "    x_predicted = model.predict(y_train)\n",
    "\n",
    "    # ! Gotta Return some parameters\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "# model.fit( truck_y_data, truck_x_data, epochs=200, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the output \n",
    "# truck_x_predicted = model.predict(truck_y_data)\n",
    "\n",
    "def evaluate_model_HDV():\n",
    "    train_model_for_HDV()\n",
    "    \n",
    "    # Display the result\n",
    "    plt.scatter(truck_x_data[::1], truck_y_data[::1], s=1)\n",
    "    plt.plot(x_predicted, truck_y_data, 'r', linewidth=4)\n",
    "    plt.title('HDV Total Weight Distribution')\n",
    "    plt.xlabel('Vehicle Total Weight')\n",
    "    plt.ylabel('Cumulative Probability')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # Extract the loss values from the training history\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    # Plot the learning curve\n",
    "    plt=reload(plt)\n",
    "    plt.plot(range(1, len(train_loss) + 1), train_loss, label='Training Loss')\n",
    "    plt.plot(range(1, len(val_loss) + 1), val_loss, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Learning Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vehicle Class and Axle Weight and Spacing Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "def axle_distribution_for_each_class_plot():\n",
    "    plt=reload(plt)\n",
    "    X = []\n",
    "    all_vehicle_classes = sorted(all_data['Class'].unique())\n",
    "    diff_no_of_axles = sorted(all_data['No of Axle'].unique())\n",
    "\n",
    "    for vehicle_class in all_vehicle_classes:\n",
    "        tmp = all_data[all_data['Class']== vehicle_class][['Total Weight','Gap Time','No of Axle']]\n",
    "        X.append(np.array(tmp['No of Axle']))\n",
    "    labels = ['Class '+ str(x) for x in all_vehicle_classes]\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    fig, ax1 = plt.subplots(1,1)\n",
    "    plt.tight_layout()\n",
    "    plt.hist(X,bins=all_vehicle_classes, density=True, histtype='bar', label=labels)\n",
    "    plt.title('Density plot for No of Axles in Each Vehicle Class')\n",
    "    plt.ylabel('Probability Density')\n",
    "    plt.xlabel('Number of Axles')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def axle_distribution_for_each_class() -> pd.DataFrame:\n",
    "    from collections import Counter\n",
    "\n",
    "    X = []\n",
    "    all_vehicle_classes = sorted(all_data['Class'].unique())\n",
    "    diff_no_of_axles = [1,2,3,4,5,6,7]\n",
    "\n",
    "    for vehicle_class in all_vehicle_classes:\n",
    "        tmp = all_data[all_data['Class']== vehicle_class][['Total Weight','Gap Time','No of Axle']]\n",
    "        X.append(np.array(tmp['No of Axle']))\n",
    "\n",
    "    df_no_of_axle = pd.DataFrame({'No of Axle':diff_no_of_axles})\n",
    "    for i in range(len(X)):\n",
    "        df_no_of_axle[all_vehicle_classes[i]] = Counter(X[i])\n",
    "        df_no_of_axle[all_vehicle_classes[i]] = df_no_of_axle[all_vehicle_classes[i]] / df_no_of_axle[all_vehicle_classes[i]].sum()\n",
    "    df_no_of_axle = df_no_of_axle.fillna(0)\n",
    "    return df_no_of_axle\n",
    "# axle_distribution_for_each_class()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding classes within HDV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#within all_data obtain subset with only HDV, with information about the class value -> count all the class values occurances and output as a distribution\n",
    "# HDV -> Class\n",
    "def finding_HDV_class_distribution(all_data):\n",
    "    from collections import Counter\n",
    "\n",
    "    HDV = all_data.loc[(all_data['Class'].astype(int) > 2)| ((all_data['Class'].astype(int) <= 2) & (all_data['Total Weight'].astype(int) > 6000))]\n",
    "    all_vehicle_classes = sorted(HDV['Class'].unique())\n",
    "    print(all_vehicle_classes)\n",
    "\n",
    "    output = pd.DataFrame({'Class': all_vehicle_classes})\n",
    "\n",
    "    # for i_class in all_vehicle_classes:\n",
    "    tmp = Counter(HDV['Class'])\n",
    "    output['HDV'] = tmp\n",
    "    output['HDV'] = output['HDV'].fillna(0)\n",
    "\n",
    "    # finding the probability. \n",
    "    sum = output['HDV'].sum()\n",
    "    output['Probabililty'] = output['HDV'] / sum\n",
    "    \n",
    "    output = np.array(output['Probabililty'])\n",
    "\n",
    "    return output\n",
    "# finding_HDV_class_distribution(all_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to find the distribution of the a discrete parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# within Key -> GVW\n",
    "def finding_distribution_from_continuous(vehicle_class_key, vehicle_class_value , total_weight_key):\n",
    "\n",
    "    # HDV = all_data.loc[(all_data[key].astype(int) == class_number) | (all_data[key].astype(str) == class_number)]\n",
    "    # all_types_within_table = sorted(HDV[key].unique())\n",
    "\n",
    "    all_types_within_class_number = sorted(HDV[class_number].unique())\n",
    "    output = pd.DataFrame({class_number: all_types_within_class_number})\n",
    "\n",
    "    # for i_class in all_vehicle_classes:\n",
    "    output[class_number] = Counter(df[class_number])\n",
    "    output[class_number] = output[class_number].fillna(0)\n",
    "\n",
    "    # finding the probability. \n",
    "    sum = output[class_number].sum()\n",
    "    output['Probabililty'] = output[class_number] / sum\n",
    "    \n",
    "    output = np.array(output['Probabililty'])\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "# finding_distribution_from_continuous(vehicle_class_key, vehicle_class_value , total_weight_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * Function Template for finding distribution from dataset, plotting the dataset, and interpolating data\n",
    "\n",
    "from importlib import reload\n",
    "import random\n",
    "\n",
    "def finding_distribution_from_continuous(vehicle_class_key, vehicle_class_value , total_weight_key):\n",
    "    # making a subset dataframe for the dataset that suit the selection\n",
    "    tmp = all_data[all_data[vehicle_class_key] == vehicle_class_value]\n",
    "    tmp = pd.DataFrame(tmp[total_weight_key].value_counts())\n",
    "\n",
    "    tmp = tmp.sort_values(total_weight_key).reset_index()\n",
    "\n",
    "    tmp.columns = [total_weight_key, 'Count']\n",
    "    total_rows = tmp['Count'].sum()\n",
    "    \n",
    "    # calculate the percentage and cumulative probability for each dataset\n",
    "    tmp['Percentage'] = tmp['Count'] / total_rows\n",
    "    tmp['Cumulative Probability'] = tmp['Percentage'].cumsum()\n",
    "    return tmp[[total_weight_key, 'Cumulative Probability']]\n",
    "\n",
    "def plot_distribution(df:pd.DataFrame):\n",
    "    import matplotlib.pyplot as plt \n",
    "    x_axis = df.columns[0]\n",
    "    y_axis = df.columns[-1]\n",
    "    plt.scatter(df[x_axis], df[y_axis], s=2)\n",
    "    # plt.plot(df[x_axis], df[y_axis])\n",
    "    plt=reload(plt)\n",
    "    plt.title(f'{x_axis} against {y_axis}')\n",
    "    plt.xlabel(x_axis)\n",
    "    plt.ylabel(y_axis)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def generate_and_interpolate_value(dataframe: pd.DataFrame):\n",
    "    x_axis = dataframe.columns[0]\n",
    "    y_axis = dataframe.columns[-1]\n",
    "\n",
    "    # Generate a random number between 0 and 1\n",
    "    random_prob = random.random()\n",
    "    # print(f'{random_prob = }')\n",
    "\n",
    "    # Sort the dataframe by cumulative probability column\n",
    "    sorted_df = dataframe.sort_values(y_axis)\n",
    "\n",
    "    # Find the closest value values based on the random probability\n",
    "    closest_values = []\n",
    "    closest_values.append(sorted_df[(sorted_df[y_axis] <= random_prob)].tail(1)[x_axis].values[0])\n",
    "    closest_values.append(sorted_df[(sorted_df[y_axis] >= random_prob)].tail(1)[x_axis].values[0])\n",
    "    # print(sorted_df[(sorted_df[y_axis] <= random_prob)].tail(1))\n",
    "    # print(sorted_df[(sorted_df[y_axis] >= random_prob)].head(1))\n",
    "    # print(closest_values)\n",
    "\n",
    "    # Calculate the value value using linear interpolation\n",
    "    interpolated_value = closest_values[0] + (closest_values[1] - closest_values[0]) * random_prob\n",
    "\n",
    "    print(f'{random_prob = }, {interpolated_value = }')\n",
    "\n",
    "    return interpolated_value\n",
    "\n",
    "# Usage:  \n",
    "        # tmp = finding_distribution_from_continuous(vehicle_class_key, vehicle_class_value , total_weight_key)\n",
    "\n",
    "# vehicle_class_key = 'Class'\n",
    "# vehicle_class_value = 2\n",
    "# total_weight_key = 'Total Weight'\n",
    "# tmp = finding_distribution_from_continuous(vehicle_class_key, vehicle_class_value , total_weight_key)\n",
    "# plot_distribution(tmp)\n",
    "# generate_and_interpolate_value(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming the columns that are unnamed (Imported)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for i in range(11,27):\n",
    "#     num = (i - 10) // 2\n",
    "#     if i % 2 == 0:\n",
    "#         new_column_name = 'Axle Spacing ' + str(num)\n",
    "#     else:\n",
    "#         new_column_name = 'Axle Weight ' + str(num)\n",
    "#     column_name = 'Unnamed: ' + str(i)\n",
    "#     all_data.rename(columns={column_name: new_column_name}, inplace=True)\n",
    "# all_data.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM for Traffic Flow Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data = pd.read_csv('Lantau Link Yearly Traffic.csv').astype(int)\n",
    "# data = data.transpose()\n",
    "# data = data.reset_index()\n",
    "\n",
    "# data = data.rename(columns={'index':'traffic_flow'})\n",
    "# data['Year'] = range(1997,2023)\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import LSTM, Dense\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# # Load the traffic data from a CSV file\n",
    "# data = pd.read_csv('Lantau Link Yearly Traffic.csv').astype(int)\n",
    "# data = data.transpose()\n",
    "# data = data.reset_index()\n",
    "\n",
    "# data = data.rename(columns={'index':'traffic_flow'})\n",
    "# data['Year'] = range(1997,2023)\n",
    "# print(data)\n",
    "\n",
    "# for j in range(5,15):\n",
    "#     # Preprocess the data\n",
    "#     scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "#     scaled_data = scaler.fit_transform(data['traffic_flow'].values.reshape(-1, 1))\n",
    "\n",
    "#     # Split the data into training and testing sets\n",
    "#     train_size = int(len(scaled_data) * 0.8)\n",
    "#     train_data = scaled_data[:train_size]\n",
    "#     test_data = scaled_data[train_size:]\n",
    "#     print(f'{train_data = }, {test_data = }')\n",
    "\n",
    "#     # Define the number of time steps to use for prediction\n",
    "#     time_steps = j\n",
    "\n",
    "#     # Function to create input sequences and corresponding labels\n",
    "#     def create_sequences(data, time_steps):\n",
    "#         X, y = [], []\n",
    "#         for i in range(len(data) - time_steps):\n",
    "#             X.append(data[i:i+time_steps])\n",
    "#             y.append(data[i+time_steps])\n",
    "#         print(len(X),len(y))\n",
    "#         return np.array(X), np.array(y)\n",
    "\n",
    "#     # Create training and testing sequences\n",
    "#     X_train, y_train = create_sequences(train_data, time_steps)\n",
    "#     X_test, y_test = create_sequences(test_data, time_steps)\n",
    "\n",
    "#     # Build the LSTM model\n",
    "#     model = Sequential()\n",
    "#     model.add(LSTM(64, input_shape=(time_steps, 1)))\n",
    "#     model.add(Dense(1))\n",
    "#     model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "#     # Set up early stopping to prevent overfitting\n",
    "#     # early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "#     # Train the model\n",
    "#     print(f'{X_train =}')\n",
    "#     print(f'{y_train =}')\n",
    "#     model.fit(X_train, y_train, epochs=100)\n",
    "#     # model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, callbacks=[early_stopping])\n",
    "\n",
    "#     # Make predictions on the test set\n",
    "#     predicted_data = model.predict(X_test)\n",
    "\n",
    "#     # Inverse transform the predictions and the actual values\n",
    "#     predicted_data = scaler.inverse_transform(predicted_data)\n",
    "#     y_test = scaler.inverse_transform(y_test)\n",
    "\n",
    "#     # Calculate the root mean squared error (RMSE)\n",
    "#     rmse = np.sqrt(np.mean((predicted_data - y_test) ** 2))\n",
    "#     print(f\"{time_steps = }Root Mean Squared Error (RMSE): {rmse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
